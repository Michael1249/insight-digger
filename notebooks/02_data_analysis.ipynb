{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"ğŸ“¦ Libraries imported successfully!\")\n",
    "print(f\"ğŸ¼ Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e416d62",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Load Data\n",
    "\n",
    "Load the data that was processed in the data ingestion notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879af53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "data_path = Path('../data/processed/loaded_data.csv')\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"âœ… Data loaded from: {data_path}\")\n",
    "    print(f\"ğŸ“Š Shape: {df.shape}\")\n",
    "else:\n",
    "    # Fallback: create demo data if no processed data exists\n",
    "    print(\"âš ï¸ No processed data found. Creating demo data...\")\n",
    "    print(\"ğŸ’¡ Run the data ingestion notebook first for your own data\")\n",
    "    \n",
    "    # Import our data connector\n",
    "    try:\n",
    "        from data_connector import load_demo_data\n",
    "        df = load_demo_data()\n",
    "        print(f\"âœ… Demo data created: {df.shape}\")\n",
    "    except ImportError:\n",
    "        # Create basic demo data if import fails\n",
    "        dates = pd.date_range('2024-01-01', periods=100)\n",
    "        df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'Category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "            'Value': np.random.randint(100, 1000, 100),\n",
    "            'Region': np.random.choice(['North', 'South'], 100)\n",
    "        })\n",
    "        print(f\"âœ… Basic demo data created: {df.shape}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nğŸ“‹ Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6f26b",
   "metadata": {},
   "source": [
    "## ğŸ” Data Exploration\n",
    "\n",
    "Let's start with basic data exploration and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcca70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"ğŸ“Š DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“ Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"ğŸ“… Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Column Information:\")\n",
    "print(\"-\" * 30)\n",
    "for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):\n",
    "    null_count = df[col].isnull().sum()\n",
    "    null_pct = (null_count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {col:<20} | {str(dtype):<10} | {null_count:3d} nulls ({null_pct:4.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1cc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and statistical summary\n",
    "print(\"ğŸ“ˆ STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numerical_cols) > 0:\n",
    "    print(f\"ğŸ”¢ Numerical columns ({len(numerical_cols)}): {list(numerical_cols)}\")\n",
    "    display(df[numerical_cols].describe())\n",
    "else:\n",
    "    print(\"ğŸ”¢ No numerical columns found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"ğŸ·ï¸ Categorical columns ({len(categorical_cols)}): {list(categorical_cols)}\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"\\nğŸ“Š {col}:\")\n",
    "        print(f\"   Unique values: {unique_count}\")\n",
    "        \n",
    "        if unique_count <= 10:  # Show values if not too many\n",
    "            value_counts = df[col].value_counts().head(10)\n",
    "            for value, count in value_counts.items():\n",
    "                pct = (count / len(df)) * 100\n",
    "                print(f\"   - {value}: {count:,} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   Top values: {df[col].value_counts().head(3).index.tolist()}\")\n",
    "else:\n",
    "    print(\"ğŸ·ï¸ No categorical columns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c288",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Data Quality Assessment\n",
    "\n",
    "Check for data quality issues like missing values, duplicates, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"âŒ Missing Values Found:\")\n",
    "    for col, count in missing_data.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Create missing data heatmap if we have missing values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "\n",
    "# Duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"\\nâŒ Duplicate rows: {duplicate_rows:,} ({(duplicate_rows/len(df)*100):.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nâœ… No duplicate rows found!\")\n",
    "\n",
    "# Data type consistency\n",
    "print(\"\\nğŸ“‹ Data Type Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check for date columns that might need conversion\n",
    "potential_date_cols = []\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if 'date' in col.lower() or 'time' in col.lower():\n",
    "        potential_date_cols.append(col)\n",
    "        \n",
    "if potential_date_cols:\n",
    "    print(f\"ğŸ“… Potential date columns: {potential_date_cols}\")\n",
    "    print(\"ğŸ’¡ Consider converting these to datetime format\")\n",
    "\n",
    "# Check for numerical columns stored as text\n",
    "potential_numeric_cols = []\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    sample_values = df[col].dropna().head(10)\n",
    "    numeric_count = 0\n",
    "    for val in sample_values:\n",
    "        try:\n",
    "            float(str(val).replace(',', '').replace('$', '').replace('%', ''))\n",
    "            numeric_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if numeric_count >= 7:  # If most values look numeric\n",
    "        potential_numeric_cols.append(col)\n",
    "        \n",
    "if potential_numeric_cols:\n",
    "    print(f\"ğŸ”¢ Potential numeric columns stored as text: {potential_numeric_cols}\")\n",
    "    print(\"ğŸ’¡ Consider converting these to numeric format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45d4a4",
   "metadata": {},
   "source": [
    "## ğŸ“Š Descriptive Analytics\n",
    "\n",
    "Perform descriptive analysis to understand patterns and distributions in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical analysis\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"ğŸ“ˆ NUMERICAL DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create distribution plots\n",
    "    fig, axes = plt.subplots(2, min(len(numerical_cols), 3), figsize=(15, 8))\n",
    "    if len(numerical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    axes = axes.flatten() if len(numerical_cols) > 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:6]):  # Max 6 plots\n",
    "        if i < len(axes):\n",
    "            # Distribution plot\n",
    "            if i < 3:\n",
    "                sns.histplot(data=df, x=col, kde=True, ax=axes[i])\n",
    "                axes[i].set_title(f'Distribution: {col}')\n",
    "            else:\n",
    "                # Box plot for second row\n",
    "                sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "                axes[i].set_title(f'Box Plot: {col}')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation analysis if we have multiple numerical columns\n",
    "    if len(numerical_cols) > 1:\n",
    "        print(\"\\nğŸ”— Correlation Analysis:\")\n",
    "        correlation_matrix = df[numerical_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, fmt='.2f')\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Highlight strong correlations\n",
    "        strong_correlations = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr) > 0.7:  # Strong correlation threshold\n",
    "                    strong_correlations.append((\n",
    "                        correlation_matrix.columns[i], \n",
    "                        correlation_matrix.columns[j], \n",
    "                        corr\n",
    "                    ))\n",
    "        \n",
    "        if strong_correlations:\n",
    "            print(\"\\nğŸ¯ Strong Correlations (|r| > 0.7):\")\n",
    "            for col1, col2, corr in strong_correlations:\n",
    "                print(f\"   {col1} â†” {col2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"\\nğŸ“Š No strong correlations found (|r| > 0.7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical analysis\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"ğŸ·ï¸ CATEGORICAL DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create count plots for categorical variables\n",
    "    n_cats = min(len(categorical_cols), 4)  # Limit to 4 plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols[:4]):\n",
    "        if df[col].nunique() <= 20:  # Only plot if not too many categories\n",
    "            value_counts = df[col].value_counts().head(10)\n",
    "            \n",
    "            # Bar plot\n",
    "            axes[i].bar(range(len(value_counts)), value_counts.values)\n",
    "            axes[i].set_title(f'Count: {col}')\n",
    "            axes[i].set_xticks(range(len(value_counts)))\n",
    "            axes[i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, v in enumerate(value_counts.values):\n",
    "                axes[i].text(j, v + max(value_counts.values) * 0.01, str(v), \n",
    "                           ha='center', va='bottom')\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f'Too many categories\\n({df[col].nunique()}) to display', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f'Categories: {col} ({df[col].nunique()})')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(categorical_cols), 4):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cross-tabulation if we have multiple categorical columns\n",
    "    if len(categorical_cols) >= 2:\n",
    "        print(\"\\nğŸ”„ Cross-tabulation Analysis:\")\n",
    "        col1, col2 = categorical_cols[0], categorical_cols[1]\n",
    "        \n",
    "        if df[col1].nunique() <= 10 and df[col2].nunique() <= 10:\n",
    "            crosstab = pd.crosstab(df[col1], df[col2], margins=True)\n",
    "            print(f\"\\nğŸ“Š {col1} vs {col2}:\")\n",
    "            display(crosstab)\n",
    "            \n",
    "            # Heatmap of cross-tabulation\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(crosstab.iloc[:-1, :-1], annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Cross-tabulation: {col1} vs {col2}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"   {col1} vs {col2}: Too many categories for detailed cross-tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c19ca",
   "metadata": {},
   "source": [
    "## ğŸ“… Time Series Analysis (if applicable)\n",
    "\n",
    "If your data has date/time columns, let's analyze trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for date columns\n",
    "date_columns = []\n",
    "for col in df.columns:\n",
    "    if 'date' in col.lower() or 'time' in col.lower():\n",
    "        date_columns.append(col)\n",
    "    else:\n",
    "        # Try to convert and see if it looks like dates\n",
    "        try:\n",
    "            sample = df[col].dropna().head(5)\n",
    "            converted = pd.to_datetime(sample, errors='coerce')\n",
    "            if not converted.isnull().all():\n",
    "                date_columns.append(col)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if date_columns:\n",
    "    print(\"ğŸ“… TIME SERIES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Found potential date columns: {date_columns}\")\n",
    "    \n",
    "    # Use the first date column\n",
    "    date_col = date_columns[0]\n",
    "    \n",
    "    try:\n",
    "        # Convert to datetime\n",
    "        df_time = df.copy()\n",
    "        df_time[date_col] = pd.to_datetime(df_time[date_col])\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Date range: {df_time[date_col].min()} to {df_time[date_col].max()}\")\n",
    "        print(f\"ğŸ“… Duration: {(df_time[date_col].max() - df_time[date_col].min()).days} days\")\n",
    "        \n",
    "        # Time series plots\n",
    "        if len(numerical_cols) > 0:\n",
    "            fig, axes = plt.subplots(min(len(numerical_cols), 2), 1, figsize=(12, 8))\n",
    "            if len(numerical_cols) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, num_col in enumerate(numerical_cols[:2]):\n",
    "                # Group by date and aggregate\n",
    "                daily_agg = df_time.groupby(df_time[date_col].dt.date)[num_col].sum().reset_index()\n",
    "                daily_agg[date_col] = pd.to_datetime(daily_agg[date_col])\n",
    "                \n",
    "                axes[i].plot(daily_agg[date_col], daily_agg[num_col], marker='o', markersize=3)\n",
    "                axes[i].set_title(f'Time Series: {num_col}')\n",
    "                axes[i].set_xlabel('Date')\n",
    "                axes[i].set_ylabel(num_col)\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Basic trend analysis\n",
    "            print(\"\\nğŸ“ˆ Trend Analysis:\")\n",
    "            for num_col in numerical_cols[:3]:  # Analyze first 3 numeric columns\n",
    "                daily_agg = df_time.groupby(df_time[date_col].dt.date)[num_col].sum()\n",
    "                \n",
    "                # Simple trend calculation\n",
    "                if len(daily_agg) > 1:\n",
    "                    trend = daily_agg.iloc[-1] - daily_agg.iloc[0]\n",
    "                    trend_pct = (trend / daily_agg.iloc[0]) * 100 if daily_agg.iloc[0] != 0 else 0\n",
    "                    \n",
    "                    direction = \"ğŸ“ˆ\" if trend > 0 else \"ğŸ“‰\" if trend < 0 else \"ğŸ“Š\"\n",
    "                    print(f\"   {direction} {num_col}: {trend:+.2f} ({trend_pct:+.1f}%) from start to end\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in time series analysis: {e}\")\n",
    "        print(\"ğŸ’¡ Check if the date column format is correct\")\n",
    "        \n",
    "else:\n",
    "    print(\"ğŸ“… No date/time columns detected for time series analysis\")\n",
    "    print(\"ğŸ’¡ If you have date data, ensure column names contain 'date' or 'time'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafc12a",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Insights Summary\n",
    "\n",
    "Let's summarize the key findings from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ KEY INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Dataset overview\n",
    "insights.append(f\"ğŸ“Š Dataset contains {df.shape[0]:,} rows and {df.shape[1]} columns\")\n",
    "\n",
    "# Data quality insights\n",
    "missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "if missing_pct > 0:\n",
    "    insights.append(f\"âŒ Data completeness: {100-missing_pct:.1f}% ({missing_pct:.1f}% missing values)\")\n",
    "else:\n",
    "    insights.append(\"âœ… Data is complete with no missing values\")\n",
    "\n",
    "# Duplicate check\n",
    "duplicate_pct = (df.duplicated().sum() / len(df)) * 100\n",
    "if duplicate_pct > 0:\n",
    "    insights.append(f\"âŒ Found {duplicate_pct:.1f}% duplicate rows\")\n",
    "else:\n",
    "    insights.append(\"âœ… No duplicate rows detected\")\n",
    "\n",
    "# Numerical insights\n",
    "if len(numerical_cols) > 0:\n",
    "    insights.append(f\"ğŸ“ˆ {len(numerical_cols)} numerical columns available for quantitative analysis\")\n",
    "    \n",
    "    # Find columns with high variability\n",
    "    high_var_cols = []\n",
    "    for col in numerical_cols:\n",
    "        cv = df[col].std() / df[col].mean() if df[col].mean() != 0 else 0\n",
    "        if cv > 1:  # Coefficient of variation > 1\n",
    "            high_var_cols.append(col)\n",
    "    \n",
    "    if high_var_cols:\n",
    "        insights.append(f\"ğŸ“Š High variability detected in: {', '.join(high_var_cols)}\")\n",
    "\n",
    "# Categorical insights\n",
    "if len(categorical_cols) > 0:\n",
    "    insights.append(f\"ğŸ·ï¸ {len(categorical_cols)} categorical columns for segmentation analysis\")\n",
    "    \n",
    "    # Find columns with many categories\n",
    "    high_card_cols = [col for col in categorical_cols if df[col].nunique() > 20]\n",
    "    if high_card_cols:\n",
    "        insights.append(f\"ğŸ” High cardinality columns (>20 categories): {', '.join(high_card_cols)}\")\n",
    "\n",
    "# Date insights\n",
    "if date_columns:\n",
    "    insights.append(f\"ğŸ“… Time series analysis available with {len(date_columns)} date column(s)\")\n",
    "\n",
    "# Print all insights\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i:2d}. {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸš€ NEXT STEPS RECOMMENDATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if missing_pct > 5:\n",
    "    recommendations.append(\"ğŸ§¹ Consider data cleaning strategies for missing values\")\n",
    "\n",
    "if len(numerical_cols) >= 2:\n",
    "    recommendations.append(\"ğŸ“Š Explore relationships between numerical variables\")\n",
    "\n",
    "if len(categorical_cols) >= 1 and len(numerical_cols) >= 1:\n",
    "    recommendations.append(\"ğŸ” Perform segmentation analysis by categories\")\n",
    "\n",
    "if date_columns:\n",
    "    recommendations.append(\"ğŸ“ˆ Conduct detailed time series analysis and forecasting\")\n",
    "\n",
    "recommendations.extend([\n",
    "    \"ğŸ“‹ Create detailed visualizations in the next notebook\",\n",
    "    \"ğŸ’¾ Save processed insights for reporting\",\n",
    "    \"ğŸ”„ Set up automated analysis pipeline\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119673a",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Export Analysis Results\n",
    "\n",
    "Save your analysis results for further use and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69826b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis summary\n",
    "analysis_summary = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values_pct': (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100,\n",
    "        'duplicate_rows': int(df.duplicated().sum()),\n",
    "        'complete_rows': int((~df.isnull().any(axis=1)).sum())\n",
    "    },\n",
    "    'column_types': {\n",
    "        'numerical': list(numerical_cols),\n",
    "        'categorical': list(categorical_cols),\n",
    "        'datetime': date_columns\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add statistical summaries\n",
    "if len(numerical_cols) > 0:\n",
    "    analysis_summary['numerical_summary'] = df[numerical_cols].describe().to_dict()\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    cat_summary = {}\n",
    "    for col in categorical_cols:\n",
    "        cat_summary[col] = {\n",
    "            'unique_values': int(df[col].nunique()),\n",
    "            'top_values': df[col].value_counts().head(5).to_dict()\n",
    "        }\n",
    "    analysis_summary['categorical_summary'] = cat_summary\n",
    "\n",
    "# Save summary as JSON\n",
    "import json\n",
    "summary_path = Path('../data/exports/analysis_summary.json')\n",
    "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"ğŸ’¾ Analysis summary saved to: {summary_path}\")\n",
    "\n",
    "# Save processed data with any cleaning\n",
    "clean_data_path = Path('../data/processed/analyzed_data.csv')\n",
    "df.to_csv(clean_data_path, index=False)\n",
    "print(f\"ğŸ’¾ Analyzed data saved to: {clean_data_path}\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete! Ready for visualization and deeper insights.\")\n",
    "print(\"ğŸ¯ Next: Open the visualization notebook to create charts and graphs.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
